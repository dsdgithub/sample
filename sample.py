

#!pip install streamlit-chat
#!pip install streamlit
#!pip install langchain
#!pip install faiss-cpu


import streamlit as st
from streamlit_chat import message
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain

# ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)
pdf_path = "C:/Users/ksrim/wiset/data/â˜…2024 ì„œìš¸êµìœ¡ ì£¼ìš”ì—…ë¬´(í™•ì •ë³¸).pdf"
loader = PyMuPDFLoader(pdf_path)
docs = loader.load()


# ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
split_documents = text_splitter.split_documents(docs)


# ë‹¨ê³„ 3: ì„ë² ë”©(Embedding) ìƒì„±
embeddings = OpenAIEmbeddings()


# ë‹¨ê³„ 4: DB ìƒì„±(Create DB) ë° ì €ì¥
# ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)


# ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
# ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
retriever = vectorstore.as_retriever()


# ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
# í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context to answer the question. 
    If you don't know the answer, just say that you don't know. 
    Answer in Korean.

    #Question: 
    {question} 
    #Context: 
    {context} 

    #Answer:"""
)

# ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
# ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
llm = ChatOpenAI(model_name="gpt-4", temperature=0)

# ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±
chain = RunnableSequence(
    {
        "context": retriever,
        "question": RunnablePassthrough()
    },
    prompt,
    llm,
    StrOutputParser()
)

# # Streamlit ì•± ì´ˆê¸° ì„¤ì •
# st.title("ì„œìš¸ì‹œ êµìœ¡ì²­ ì±—ë´‡")
# st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì±—ë´‡ì´ ë‹µë³€í•©ë‹ˆë‹¤.")

# # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
# if question:
#     response = chain.invoke(question)
#     st.write("ë‹µë³€:", response)



chain = ConversationalRetrievalChain.from_llm(llm = ChatOpenAI(temperature=0.0,model_name='gpt-4'), retriever=retriever)




st.set_page_config(
    page_title="êµìœ¡TALK", # í¬ë¡¬ì°½ ìƒë‹¨ì— ë³´ì¼ ì œëª©
    page_icon="ğŸ«",    # í¬ë¡¬ì°½ ìƒë‹¨ì— ë³´ì¼ ì•„ì´ì½˜ ëª¨ì–‘ ì„¤ì •
    
)



# ê¸°ì—… ë§ˆí¬ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì • (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” URL)
logo = "C:/Users/ksrim/wiset/data/seoultalk.png"  # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ

# í™”ë©´ ìƒë‹¨ì— ê¸°ì—… ë§ˆí¬ ì´ë¯¸ì§€ ì‚½ì…
st.image(logo, width=700)





def conversational_chat(query):  #ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ê³¼ê±° ëŒ€í™” ì €ì¥ ì´ë ¥ì— ëŒ€í•œ ì²˜ë¦¬      
    result = chain({"question": query, "chat_history": st.session_state['history']})
    st.session_state['history'].append((query, result["answer"]))        
    return result["answer"]
    
if 'history' not in st.session_state:
    st.session_state['history'] = []

if 'generated' not in st.session_state:
    st.session_state['generated'] = ["ì„œìš¸êµìœ¡ ì£¼ìš” ì—…ë¬´ì— ê´€í•´ ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”!ğŸ«"]


if 'past' not in st.session_state:
    st.session_state['past'] = ["ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ìˆìŠµë‹ˆë‹¤."]



        
#ì±—ë´‡ ì´ë ¥ì— ëŒ€í•œ ì»¨í…Œì´ë„ˆ
response_container = st.container()

#ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì— ëŒ€í•œ ì»¨í…Œì´ë„ˆ
container = st.container()

with container: #ëŒ€í™” ë‚´ìš© ì €ì¥(ê¸°ì–µ)
    with st.form(key='Conv_Question', clear_on_submit=True):           
        user_input = st.text_input("ì§ˆë¬¸:", placeholder="2024ë…„ ì„œìš¸êµìœ¡ ì£¼ìš”ì—…ë¬´ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš” (:", key='input')
        submit_button = st.form_submit_button(label='Send')
            
    if submit_button and user_input:
        output = conversational_chat(user_input)
            
        st.session_state['past'].append(user_input)
        st.session_state['generated'].append(output)

if st.session_state['generated']:
    with response_container:
        for i in range(len(st.session_state['generated'])):
            message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style = "fun-emoji", seed = "Nala")
            message(st.session_state["generated"][i], key=str(i), avatar_style = "bottts", seed = "Fluffy")




# í•˜ë‹¨ì— í…ìŠ¤íŠ¸ ì¶”ê°€
footer = """
    <style>
    .footer {
        position: fixed;
        left: 0;
        bottom: 30px;  /* í™”ë©´ í•˜ë‹¨ì—ì„œ 20px ìœ„ë¡œ ì˜¬ë¦¼ */
        width: 100%;
        background-color: white;
        color: black;
        text-align: center;
        padding: 10px;
        font-size: 18px;  /* ê¸€ì”¨ í¬ê¸° ì¡°ì • */
    }
    </style>
    <div class="footer">
         ë¬¸ì˜ì „í™” ì •ë³´í™”ë‹´ë‹¹ê´€ â˜ï¸ 02-0000-0000
    </div>
    """

st.markdown(footer, unsafe_allow_html=True)